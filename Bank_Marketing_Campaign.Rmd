---
title: "Bank_Marketing_Campaign"
author: "Amanda Peralta"
output:
  pdf_document: default
  html_document: default
---

```{r,include=FALSE,message=FALSE,warning=FALSE}

# Loaded packages that will be used for analysis and clustering

library(readr); library(reshape2)
library(rpart); library(e1071)
library(randomForest); library(ggplot2)
library(tidyverse); library(caret)
library(glmnet); library(ggpubr)
library(dplyr); library(cluster)
library(dplyr); library(ggplot2)
library(readr); library(Rtsne)
library(DescTools); library(corrplot)
library(naivebayes); library(adabag)
library(nnet)

```



```{r,message=FALSE,include=FALSE}

# Read in data that was obtained from the UCI repository
Bank.Ori<- read_delim("bank-full.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
Bank.Ori = mutate_if(Bank.Ori, is.character, as.factor)

# only used a subset of the data
set.seed(1)
samp= sample(1:45211,15000,replace = FALSE)
Bank = Bank.Ori[samp,]
Bank = Bank[-11773,]  # remove observation that seemed to be a result of human error
Bank.Scaled = Bank
Bank.Scaled[,c(1,6,10,12,13,14,15)] = scale(Bank.Scaled[,c(1,6,10,12,13,14,15)] , center = TRUE, scale = TRUE)



```




```{r}


## is there an imbalance of subscriptions an non subscriptions
## although there is some there is not enough to be of concern  
summary(Bank)
sum(Bank$y=="no")/dim(Bank)[1]
sum(Bank$y=="yes")/dim(Bank)[1]



```



```{r,echo=FALSE}
# histograms of the following variables
# age job marital education default balance  housing 
# loan contact day month duration  campaign pdays
# previous poucome 

# age
h.age = ggplot(Bank) + aes(x = age, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# job
h.job = ggplot(Bank) + aes(x = job, fill = y) + geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1,size=rel(0.75))) +scale_y_continuous(labels = scales::percent, name = "Percent")
# marital
h.marital = ggplot(Bank) + aes(x = marital, fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent")
# education
h.edu = ggplot(Bank) + aes(x = education, fill = y) + geom_bar(aes(y = (..count..)/sum(..count..)))  +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1,size=rel(0.75)))
# defaulta
h.default = ggplot(Bank) + aes(x = default, fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent")
# balance
h.balance = ggplot(Bank) + aes(x = balance, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# housing
h.housing = ggplot(Bank) + aes(x = housing, fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent")
# loan
h.loan = ggplot(Bank) + aes(x = loan, fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent")
# contact
h.contact = ggplot(Bank) + aes(x = contact, fill = y) + geom_bar(aes(y = (..count..)/sum(..count..)))  +
 scale_fill_brewer(palette = "Paired") + theme_minimal() + theme(axis.text.x = element_text(size=rel(0.75)))+scale_y_continuous(labels = scales::percent, name = "Percent")

#day
h.day = ggplot(Bank) + aes(x = day, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# month
h.month = ggplot(Bank) + aes(x = factor(month,level=c('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec' )   ), fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +scale_fill_brewer(palette = "Paired") + theme_minimal()+xlab("Month")+scale_y_continuous(labels = scales::percent, name = "Percent")
# duration
h.duration = ggplot(Bank) + aes(x = duration, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# campaign
h.camp = ggplot(Bank) + aes(x = campaign, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# pdays
h.pday = ggplot(Bank) + aes(x = pdays, fill = y) + geom_histogram(bins = 30L) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()
# previous
h.prev = ggplot(Bank) + aes(x = previous, fill = y) + geom_histogram(bins = 30L) +
  scale_fill_brewer(palette = "Paired")+ theme_minimal()
#poutcome
h.pout = ggplot(Bank) + aes(x = poutcome, fill = y) +  geom_bar(aes(y = (..count..)/sum(..count..))) +
 scale_fill_brewer(palette = "Paired") + theme_minimal()+scale_y_continuous(labels = scales::percent, name = "Percent")






# Histograms of the variables in sets of 2

ggarrange( h.age,h.balance, ncol = 1, nrow = 2)
ggarrange( h.day,h.duration, ncol = 1, nrow = 2)
ggarrange( h.camp,h.pday, ncol = 1, nrow = 2)
ggarrange( h.prev,h.job, ncol = 1, nrow = 2)

ggarrange( h.marital,h.edu, ncol = 1, nrow = 2)
ggarrange( h.default,h.housing, ncol = 1, nrow = 2)
ggarrange( h.loan,h.contact, ncol = 1, nrow = 2)
ggarrange( h.month,h.pout, ncol = 1, nrow = 2)




```




```{r}
# check which is numeric / categorical
quantitatives = c(1,6,10,12,13,14,15)
num = Bank.Scaled[,quantitatives] 

# Created a correlation plot to determine if there is a need for model selection
correlation = round(cor( num ),2)
melt.cor= melt(correlation)

ggplot(melt.cor) +
 aes(x = Var1, y = Var2, fill = value) +
 geom_tile(size = 1L) +
 scale_fill_distiller(palette = "BuPu",trans = "reverse") +
 theme_dark()




```





```{r}
# boxplots for the variables age, balance, day, duration, campaign, pday, and previous

bp.age = ggplot(Bank) + aes(x = y, y = age) + geom_boxplot(fill = "#6baed6") + theme_minimal()
bp.balance = ggplot(Bank) + aes(x = y, y = balance) + geom_boxplot(fill = "#6baed6") + theme_minimal()
bp.day = ggplot(Bank) + aes(x = y, y = day) + geom_boxplot(fill = "#6baed6") + theme_minimal()

bp.duration = ggplot(Bank) + aes(x = y, y = duration) + geom_boxplot(fill = "#6baed6") + theme_minimal()
bp.camp = ggplot(Bank) + aes(x = y, y = campaign) + geom_boxplot(fill = "#6baed6") + theme_minimal()

bp.pday = ggplot(Bank) + aes(x = y, y = pdays) + geom_boxplot(fill = "#6baed6") + theme_minimal()
bp.prev = ggplot(Bank) + aes(x = y, y = previous) + geom_boxplot(fill = "#6baed6") + theme_minimal()


ggarrange( bp.age,bp.balance, bp.day, bp.duration, ncol = 2, nrow = 2)
ggarrange( bp.camp,bp.pday,bp.prev,  ncol = 2, nrow = 2)



```



```{r}
 # Scatterplots for age, day, pdays
ggplot(Bank) + aes(x = age, y = balance) + geom_point(size = 1L, colour = "#9ecae1") + theme_minimal()
ggplot(Bank) + aes(x = day, y = campaign) + geom_point(size = 1L, colour = "#9ecae1") + theme_minimal()
ggplot(Bank) + aes(x = pdays, y = previous) + geom_point(size = 1L, colour = "#9ecae1") + theme_minimal()



```



```{r}
# prepping for K-medoids clustering
# determines gowers distance
df=Bank.Scaled
gower_dist <- daisy(df, metric = "gower")



```



```{r}

# ran PAM algorithm
sil_width <- c(NA)
for(i in 6:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
plot(1:8, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:8, sil_width)

# the result of the first loop
# c(NA, 0.1507379, 0.1427649, 0.1473079, 0.1394863, 0.1505739, 0.1507535, 0.1508811)



```





```{r}

# best k's are k=2 and k=6

k <- 2
pam_fit <- pam(gower_dist, diss = TRUE, k)
pam_results <- df %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results$the_summary

k6 <- 6
pam_fit6 <- pam(gower_dist, diss = TRUE, k)
pam_results6 <- df %>%
  mutate(cluster = pam_fit6$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results6$the_summary



```



```{r}

# 2-d visual of the clusters using the function Rtsne to do so

tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))
ggplot(aes(x = X, y = Y), data = tsne_data) +geom_point(aes(color = cluster))

tsne_data6 <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit6$clustering))
ggplot(aes(x = X, y = Y), data = tsne_data) +geom_point(aes(color = cluster))



```





```{r}
# pairwise scatterplots for numeric variables.
plot(Bank.Scaled[,quantitatives],col=ifelse(Bank.Scaled$y=="no", "darkslateblue ","darkorchid1 "))



```




```{r}
# prepping for cross-validation with variable duration

#### Get Tuning Parameters for svm
set.seed(1)
train = sample(x=nrow(Bank.Scaled), size = 1000)

# SVM
set.seed(1)
tune.out = tune(svm, y ~ ., data = Bank.Scaled[train, ], kernel = "radial", ranges = list(cost=c(0.1, 1, 10, 100, 1000), gamma = c(0.1, 0.5, 1, 2, 3, 4)))

best.cost = tune.out$best.parameters[1,1]## 10
best.gamma = tune.out$best.parameters[1,2]## 1

# Decision Trees
set.seed(1)
tune.dtree = tune.rpart(formula = y~., data = Bank.Scaled[train, ],
minsplit=seq(from=60, to=100, by=5), cp = seq(from=0.01, to=0.2, by = 0.01)) 

best.minsplit  = tune.dtree$best.parameters[1,1]## 65
best.cp  = tune.dtree$best.parameters[1,2]## 0.01


# Random Forest

set.seed(1)  
tune.rf = tune.randomForest(x = Bank.Scaled[train, 1:16], y = Bank.Scaled$y[train], formula = salary~., data = Bank.Scaled[train, ],
mtry = seq(from=2, to=6, by=1), ntree = seq(from=10, to= 100, by=5)) 

best.mtry = tune.rf$best.parameters[1,1]
best.ntree = tune.rf$best.parameters[1,2]


```










```{r}


CV.Bank = Bank.Scaled

k = 10
n = dim(Bank.Scaled)[1]  #14999
set.seed(7)
samp.index = sample(1:n,n,replace=F)
rows = k
cols = floor(n/k)
samp.matrix = matrix(samp.index[1:(cols*rows)],nrow=rows)
dim(samp.matrix) 
me =matrix(0,7,k)



for(j in 1:k){
		test = CV.Bank[samp.matrix[j,],]
		train = CV.Bank[-samp.matrix[j,],]
		x = train[,-17]
		y = train$y
		x1 = test[,-17]
		y.test = test$y


		### SVM
    svm.cv<- svm(y~., data =  data.frame(y,x) ,kernel = "radial", cost = best.cost, gamma = best.gamma)
    svm.cv.p <- predict(svm.cv, data.frame(x1))
    me[1,j] = sum(diag(table(svm.cv.p,y.test)))

    ### Random Forest
    set.seed(j)
    rf.cv = randomForest(y~. ,data = data.frame(cbind(y,x)), ntree=best.ntree,mtry= best.mtry)
    rf.cv.p = predict(rf.cv, data.frame(x1))
    me[2,j] = sum(diag(table(rf.cv.p,y.test)))

    ### Decision Trees
    set.seed(j)
    tree.cv = rpart(y ~., data = cbind(y,x), method="class",minsplit = 60,cp= best.cp)
    tree.cv.p = predict(tree.cv, x1, type = "class")
    me[3,j] = sum(diag(table(tree.cv.p, y.test)))


    ### logistic regression
    glm.cv <- glm(y ~. , data = cbind(y,x) , family =binomial)
    glm.probs.cv <- predict(glm.cv,x1 ,type = "response")
    glm.cv.p <- rep("no ", cols)
    glm.cv.p[glm.probs.cv > .5] <- "yes"
    me[4,j] = sum(diag(table(glm.cv.p, y.test)))


    ### naive bayes
    nb.cv = naive_bayes(x,y)
    nb.probs.cv = predict(nb.cv,data.frame(x1),laplace=0)
    me[5,j] = sum(diag(table(nb.probs.cv,y.test)))

    
    ### adaptive boosting 
    set.seed(j)
    ab.cv = boosting(y~.,data=data.frame(y,x),mfinal=10)
    ab.cv.p = predict.boosting(ab.cv,newdata = data.frame(y.test,x1),newmfinal = 10)
    me[6,j] = sum(diag(table(ab.cv.p$class,y.test)))
    
    
    ## neural nets
    nn.cv = nnet(y~.,data=data.frame(y,x),size=4,decay=0.0001,maxit=10000)
    nn.probs.cv = predict(nn.cv,x1,type="class")
    me[7,j] = sum(diag(table(nn.probs.cv,y.test)))



}


final.accurs=me

row.names(final.accurs)=c("SVM","RF","DT","LR","NB","AB","NN")
mean.final.accurs = apply(me/cols,1,mean)
sd.final.accurs = apply(me/cols,1,sd)
output = data.frame("Mean.CV.Accur" = mean.final.accurs, "LB" = (mean.final.accurs - sd.final.accurs/sqrt(k)), "UB" = (mean.final.accurs + sd.final.accurs/sqrt(k)))

output
  
```











```{r}

# Visual of the results from cross-validating
# CI and mean
ggplot(data = output, mapping = aes(x = rownames(final.accurs), y = mean.final.accurs, 
color = rownames(final.accurs))) + 
  
geom_point()  + xlab("Models") + ylab("CV Accuracy Rate") + 
  
geom_errorbar(aes(ymin = mean.final.accurs - sd.final.accurs/sqrt(k)), ymax = (mean.final.accurs + sd.final.accurs/sqrt(k)), width=.2, position=position_dodge(0.05)) + 
  
ggtitle("Cross-Validated Accuracy Rate for Each Model") + ylim(0.865, 0.905)+ labs(color='Model') 
  

```




```{r,eval=FALSE}
# prepping for cross-validation without variable duration
#### Get Tuning Parameters for svm, DT, and RF
set.seed(1)
trainD = sample(x=nrow(Bank.Scaled), size = 1000)

# SVM
set.seed(1)
tune.out = tune(svm, y ~ ., data = Bank.Scaled[trainD, -12], kernel = "radial", 
                ranges = list(cost=c(0.1, 1, 10, 100, 1000), gamma = c(0.1, 0.5, 1, 2, 3, 4)))

best.cost = tune.out$best.parameters[1,1]
best.gamma = tune.out$best.parameters[1,2]

# Decision Trees
set.seed(1)
tune.dtree = tune.rpart(formula = y~., data = Bank.Scaled[trainD,-12 ],
minsplit=seq(from=60, to=100, by=5), cp = seq(from=0.01, to=0.2, by = 0.01)) 

best.minsplit  = tune.dtree$best.parameters[1,1]
best.cp  = tune.dtree$best.parameters[1,2]


# Random Forest

set.seed(1)  
tune.rf = tune.randomForest(x = Bank.Scaled[trainD, -c(12,17)], y = Bank.Scaled$y[trainD], 
                            formula = salary~., data = Bank.Scaled[trainD,-12],
mtry = seq(from=2, to=6, by=1), ntree = seq(from=10, to= 100, by=5)) 

best.mtry = tune.rf$best.parameters[1,1]
best.ntree = tune.rf$best.parameters[1,2]
```




```{r,eval=FALSE}

## 10-fold cross-validation
CV.Bank = Bank.Scaled

k = 10
n = dim(Bank.Scaled)[1]  #14999
set.seed(7)
samp.index = sample(1:n,n,replace=F)
rows = k
cols = floor(n/k)
samp.matrix = matrix(samp.index[1:(cols*rows)],nrow=rows)
dim(samp.matrix) 
me =matrix(0,7,k)



for(j in 1:k){
		test = CV.Bank[samp.matrix[j,],]
		train = CV.Bank[-samp.matrix[j,],]
		x = train[,-c(12,17)]
		y = train$y
		x1 = test[,-c(12,17)]
		y.test = test$y


		### SVM
    svm.cv<- svm(y~., data =  data.frame(y,x) ,kernel = "radial", cost = best.cost,
                 gamma = best.gamma)
    svm.cv.p <- predict(svm.cv, data.frame(x1))
    me[1,j] = sum(diag(table(svm.cv.p,y.test)))

    ### Random Forest
    set.seed(j)
    rf.cv = randomForest(y~. ,data = data.frame(cbind(y,x)), ntree=best.ntree,
                         mtry= best.mtry)
    rf.cv.p = predict(rf.cv, data.frame(x1))
    me[2,j] = sum(diag(table(rf.cv.p,y.test)))

    ### Decision Trees
    set.seed(j)
    tree.cv = rpart(y ~., data = cbind(y,x), method="class",minsplit = 60,cp= best.cp)
    tree.cv.p = predict(tree.cv, x1, type = "class")
    me[3,j] = sum(diag(table(tree.cv.p, y.test)))


    ### logistic regression
    glm.cv <- glm(y ~. , data = cbind(y,x) , family =binomial)
    glm.probs.cv <- predict(glm.cv,x1 ,type = "response")
    glm.cv.p <- rep("no ", cols)
    glm.cv.p[glm.probs.cv > .5] <- "yes"
    me[4,j] = sum(diag(table(glm.cv.p, y.test)))


    ### naive bayes
    nb.cv = naive_bayes(x,y)
    nb.probs.cv = predict(nb.cv,data.frame(x1),laplace=0)
    me[5,j] = sum(diag(table(nb.probs.cv,y.test)))

    
    ### adaptive boosting 
    set.seed(j)
    ab.cv = boosting(y~.,data=data.frame(y,x),mfinal=10)
    ab.cv.p = predict.boosting(ab.cv,newdata = data.frame(y.test,x1),newmfinal = 10)
    me[6,j] = sum(diag(table(ab.cv.p$class,y.test)))
    
    
    ## neural nets
    nn.cv = nnet(y~.,data=data.frame(y,x),size=4,decay=0.0001,maxit=10000)
    nn.probs.cv = predict(nn.cv,x1,type="class")
    me[7,j] = sum(diag(table(nn.probs.cv,y.test)))



}


final.accurs=me

row.names(final.accurs)=c("SVM","RF","DT","LR","NB","AB","NN")

## finding mean and SD of resulting
## accuracy rates for each cv
mean.final.accurs = apply(me/cols,1,mean)
sd.final.accurs = apply(me/cols,1,sd)
output = data.frame("Mean.CV.Accur" = mean.final.accurs, 
                    "LB" = (mean.final.accurs - sd.final.accurs/sqrt(k)), 
                    "UB" = (mean.final.accurs + sd.final.accurs/sqrt(k)))

output
  

```







```{r,eval=FALSE}
# Visual of the results from cross-validating
# CI and mean
ggplot(data = output, mapping = aes(x = rownames(final.accurs), y = mean.final.accurs, 
color = rownames(final.accurs))) + 
  
geom_point()  + xlab("Models") + ylab("CV Accuracy Rate") + 
  
geom_errorbar(aes(ymin = mean.final.accurs - sd.final.accurs/sqrt(k)),
              ymax = (mean.final.accurs + sd.final.accurs/sqrt(k)), width=.2,
              position=position_dodge(0.05)) + 
  
ggtitle("Cross-Validated Accuracy Rate for Each Model") + ylim(0.835, 0.895)+ labs(color='Model') 
  

```













